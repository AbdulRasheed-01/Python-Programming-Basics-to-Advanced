Week 5: File Handling and Exception Management
üìã Learning Objectives
By the end of this week, you will be able to:

‚úÖ Read from and write to files in various formats

‚úÖ Handle exceptions gracefully to prevent crashes

‚úÖ Work with JSON data for configuration and storage

‚úÖ Process CSV files for data analysis

‚úÖ Use context managers (with statement) for resource management

‚úÖ Create custom exception classes

‚úÖ Implement logging for debugging and monitoring

‚úÖ Work with binary files and different encodings

üìö Theory Content

5.1 File Formats Comparison

Format	   |       Use Case	              |      Pros	                   |   Cons

TXT	       |       Simple text	          |      Universal,            |       simple	No structure

CSV	       |       Tabular data	          |      Excel compatible,small	 |     Limited data types

JSON	     |       Configuration, APIs	  |      Human-readable, nested	|      Verbose

XML	       |       Legacy systems	        |      Schema validation	       |   Verbose, complex

YAML	     |       Configuration	        |      Very readable	         |     Whitespace sensitive

Binary	   |       Performance	          |      Compact, fast	         |     Not human-readable

üîß Practical Exercises

Exercise 1: Basic File Operations
  # file_basics.py
  """
  Basic file reading and writing operations
  """
  
  print("="*60)
  print("BASIC FILE OPERATIONS")
  print("="*60)
  
  import os
  
  # 1. Writing to a file
  print("\n1. WRITING TO A FILE:")
  print("-"*40)
  
  # Using write() method
  with open("sample.txt", "w") as file:
      file.write("Hello, World!\n")
      file.write("This is line 2.\n")
      file.write("This is line 3.\n")
  print("‚úÖ File 'sample.txt' created and written")
  
  # Writing multiple lines
  lines = ["First line", "Second line", "Third line", "Fourth line"]
  with open("multiple.txt", "w") as file:
      for line in lines:
          file.write(line + "\n")
  print("‚úÖ File 'multiple.txt' created with multiple lines")
  
  # 2. Reading from a file
  print("\n2. READING FROM A FILE:")
  print("-"*40)
  
  # Read entire file
  with open("sample.txt", "r") as file:
      content = file.read()
      print("Full content:")
      print(content)
  
  # Read line by line
  print("\nReading line by line:")
  with open("sample.txt", "r") as file:
      for line in file:
          print(f"  Line: {line.strip()}")
  
  # Read all lines into list
  with open("multiple.txt", "r") as file:
      lines = file.readlines()
      print(f"\nLines as list: {lines}")
      print(f"Number of lines: {len(lines)}")
  
  # 3. Appending to a file
  print("\n3. APPENDING TO A FILE:")
  print("-"*40)
  
  with open("sample.txt", "a") as file:
      file.write("This line is appended.\n")
      file.write("Another appended line.\n")
  print("‚úÖ Content appended to 'sample.txt'")
  
  # Verify append worked
  with open("sample.txt", "r") as file:
      print("Updated content:")
      print(file.read())
  
  # 4. File modes demonstration
  print("\n4. FILE MODES:")
  print("-"*40)
  
  # 'r' - read (default)
  try:
      with open("nonexistent.txt", "r") as file:
          content = file.read()
  except FileNotFoundError as e:
      print(f"'r' mode: File not found error - {e}")
  
  # 'w' - write (creates new or truncates)
  with open("new_file.txt", "w") as file:
      file.write("This file was created with 'w' mode")
  print("'w' mode: Created new_file.txt")
  
  # 'x' - exclusive creation (fails if exists)
  try:
      with open("new_file.txt", "x") as file:
          file.write("This will fail")
  except FileExistsError as e:
      print(f"'x' mode: File exists error - {e}")
  
  # 'a' - append
  with open("new_file.txt", "a") as file:
      file.write("\nThis line was appended")
  print("'a' mode: Appended to new_file.txt")
  
  # 'r+' - read and write
  with open("new_file.txt", "r+") as file:
      content = file.read()
      file.seek(0)  # Go back to beginning
      file.write("MODIFIED: " + content)
  print("'r+' mode: Read and modified file")
  
  # 5. File positioning
  print("\n5. FILE POSITIONING (seek/tell):")
  print("-"*40)
  
  with open("sample.txt", "r") as file:
      print(f"Initial position: {file.tell()}")
      
      content = file.read(10)
      print(f"Read 10 chars: '{content}'")
      print(f"Position after read: {file.tell()}")
      
      file.seek(0)
      print(f"After seek(0): {file.tell()}")
      
      file.seek(5)
      print(f"After seek(5): {file.tell()}")
      content = file.read(5)
      print(f"Read from position 5: '{content}'")
  
  # 6. Working with file paths
  print("\n6. FILE PATHS:")
  print("-"*40)
  
  import os.path
  
  # Get current directory
  current_dir = os.getcwd()
  print(f"Current directory: {current_dir}")
  
  # Join paths
  file_path = os.path.join(current_dir, "sample.txt")
  print(f"Full path: {file_path}")
  
  # Check if file exists
  print(f"File exists: {os.path.exists('sample.txt')}")
  print(f"Is file: {os.path.isfile('sample.txt')}")
  print(f"Is directory: {os.path.isdir('sample.txt')}")
  
  # File info
  file_size = os.path.getsize("sample.txt")
  print(f"File size: {file_size} bytes")
  print(f"Last modified: {os.path.getmtime('sample.txt')}")
  
  # 7. Working with directories
  print("\n7. DIRECTORY OPERATIONS:")
  print("-"*40)
  
  # Create directory
  os.makedirs("test_dir/sub_dir", exist_ok=True)
  print("‚úÖ Created test_dir/sub_dir")
  
  # List directory contents
  print("\nFiles in current directory:")
  for item in os.listdir("."):
      if os.path.isfile(item):
          print(f"  üìÑ {item}")
      elif os.path.isdir(item):
          print(f"  üìÅ {item}/")
  
  # Walk through directory tree
  print("\nDirectory tree:")
  for root, dirs, files in os.walk("test_dir"):
      level = root.count(os.sep)
      indent = "  " * level
      print(f"{indent}üìÅ {os.path.basename(root)}/")
      for file in files:
          print(f"{indent}  üìÑ {file}")
  
  # 8. Temporary files
  print("\n8. TEMPORARY FILES:")
  print("-"*40)
  
  import tempfile
  
  # Create temporary file
  with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp:
      temp.write("Temporary data")
      temp_name = temp.name
      print(f"Created temp file: {temp_name}")
  
  # Read temp file
  with open(temp_name, 'r') as f:
      print(f"Temp file content: {f.read()}")
  
  # Clean up
  os.unlink(temp_name)
  print("Temp file deleted")
  
  # 9. File encoding
  print("\n9. FILE ENCODING:")
  print("-"*40)
  
  # Write with different encoding
  text = "Caf√© M√ºller Âåó‰∫¨ Êù±‰∫¨"
  
  with open("utf8.txt", "w", encoding="utf-8") as f:
      f.write(text)
  print("‚úÖ Written with UTF-8 encoding")
  
  with open("utf16.txt", "w", encoding="utf-16") as f:
      f.write(text)
  print("‚úÖ Written with UTF-16 encoding")
  
  # Read back
  with open("utf8.txt", "r", encoding="utf-8") as f:
      content = f.read()
      print(f"UTF-8 read: {content}")
  
  # 10. Context manager custom class
  print("\n10. CUSTOM CONTEXT MANAGER:")
  print("-"*40)
  
  class FileManager:
      """Custom context manager for file handling"""
      
      def __init__(self, filename, mode):
          self.filename = filename
          self.mode = mode
          self.file = None
      
      def __enter__(self):
          print(f"Opening {self.filename} in {self.mode} mode")
          self.file = open(self.filename, self.mode)
          return self.file
      
      def __exit__(self, exc_type, exc_val, exc_tb):
          print(f"Closing {self.filename}")
          if self.file:
              self.file.close()
          if exc_type:
              print(f"An exception occurred: {exc_val}")
          return True  # Suppress exceptions
  
  # Use custom context manager
  with FileManager("test_custom.txt", "w") as f:
      f.write("Testing custom context manager")
      print("File written")
  
  # Clean up
  print("\nCleaning up test files...")
  for f in ["sample.txt", "multiple.txt", "new_file.txt", "utf8.txt", "utf16.txt", "test_custom.txt"]:
      if os.path.exists(f):
          os.remove(f)
          print(f"Removed {f}")
  
  # Remove test directory
  import shutil
  if os.path.exists("test_dir"):
      shutil.rmtree("test_dir")
      print("Removed test_dir")
Output:

============================================================
BASIC FILE OPERATIONS
============================================================

1. WRITING TO A FILE:
----------------------------------------
‚úÖ File 'sample.txt' created and written
‚úÖ File 'multiple.txt' created with multiple lines

2. READING FROM A FILE:
----------------------------------------
Full content:
Hello, World!
This is line 2.
This is line 3.

Reading line by line:
  Line: Hello, World!
  Line: This is line 2.
  Line: This is line 3.

Lines as list: ['First line\n', 'Second line\n', 'Third line\n', 'Fourth line\n']
Number of lines: 4

3. APPENDING TO A FILE:
----------------------------------------
‚úÖ Content appended to 'sample.txt'
Updated content:
Hello, World!
This is line 2.
This is line 3.
This line is appended.
Another appended line.

4. FILE MODES:
----------------------------------------
'r' mode: File not found error - [Errno 2] No such file or directory: 'nonexistent.txt'
'w' mode: Created new_file.txt
'x' mode: File exists error - [Errno 17] File exists: 'new_file.txt'
'a' mode: Appended to new_file.txt
'r+' mode: Read and modified file

5. FILE POSITIONING (seek/tell):
----------------------------------------
Initial position: 0
Read 10 chars: 'Hello, Wor'
Position after read: 10
After seek(0): 0
After seek(5): 5
Read from position 5: ', Wor'

6. FILE PATHS:
----------------------------------------
Current directory: /home/user/ros2_ws
Full path: /home/user/ros2_ws/sample.txt
File exists: True
Is file: True
Is directory: False
File size: 98 bytes
Last modified: 1705412345.6789

7. DIRECTORY OPERATIONS:
----------------------------------------
‚úÖ Created test_dir/sub_dir

Files in current directory:
  üìÑ file_basics.py
  üìÑ sample.txt
  üìÑ multiple.txt
  üìÑ new_file.txt
  üìÅ test_dir/

Directory tree:
üìÅ test_dir/
  üìÅ sub_dir/

8. TEMPORARY FILES:
----------------------------------------
Created temp file: /tmp/tmpabcdefg
Temp file content: Temporary data
Temp file deleted

9. FILE ENCODING:
----------------------------------------
‚úÖ Written with UTF-8 encoding
‚úÖ Written with UTF-16 encoding
UTF-8 read: Caf√© M√ºller Âåó‰∫¨ Êù±‰∫¨

10. CUSTOM CONTEXT MANAGER:
----------------------------------------
Opening test_custom.txt in w mode
File written
Closing test_custom.txt

Cleaning up test files...
Removed sample.txt
Removed multiple.txt
Removed new_file.txt
Removed utf8.txt
Removed utf16.txt
Removed test_custom.txt
Removed test_dir

Exercise 2: Exception Handling
# exceptions.py
"""
Comprehensive exception handling in Python
"""

print("="*60)
print("EXCEPTION HANDLING")
print("="*60)

# 1. Basic try-except
print("\n1. BASIC TRY-EXCEPT:")
print("-"*40)

try:
    number = int(input("Enter a number: "))
    result = 10 / number
    print(f"10 / {number} = {result}")
except ValueError:
    print("‚ùå That's not a valid number!")
except ZeroDivisionError:
    print("‚ùå Cannot divide by zero!")
except Exception as e:
    print(f"‚ùå An unexpected error occurred: {e}")

# 2. Multiple exceptions in one block
print("\n2. MULTIPLE EXCEPTIONS:")
print("-"*40)

try:
    num = int(input("Enter index: "))
    my_list = [1, 2, 3]
    print(f"Element at index {num}: {my_list[num]}")
except (ValueError, IndexError) as e:
    print(f"‚ùå Error: {e} - Invalid input or index")

# 3. try-except-else
print("\n3. TRY-EXCEPT-ELSE:")
print("-"*40)

try:
    num = int(input("Enter a positive number: "))
    if num <= 0:
        raise ValueError("Number must be positive")
except ValueError as e:
    print(f"‚ùå Error: {e}")
else:
    print(f"‚úÖ Success! You entered {num}")
    print(f"   Square root: {num ** 0.5:.2f}")

# 4. try-except-finally
print("\n4. TRY-EXCEPT-FINALLY:")
print("-"*40)

file = None
try:
    file = open("test_file.txt", "r")
    content = file.read()
    print(f"File content: {content}")
except FileNotFoundError:
    print("‚ùå File not found!")
finally:
    if file:
        file.close()
        print("‚úÖ File closed in finally block")
    print("üîÑ This always executes")

# 5. Raising exceptions
print("\n5. RAISING EXCEPTIONS:")
print("-"*40)

def validate_age(age):
    """Validate age with custom exceptions"""
    if not isinstance(age, (int, float)):
        raise TypeError("Age must be a number")
    if age < 0:
        raise ValueError("Age cannot be negative")
    if age > 150:
        raise ValueError("Age cannot be > 150")
    return True

# Test validation
test_ages = [25, -5, "twenty", 200]
for age in test_ages:
    try:
        validate_age(age)
        print(f"‚úÖ Age {age} is valid")
    except (TypeError, ValueError) as e:
        print(f"‚ùå Age {age}: {e}")

# 6. Custom exception classes
print("\n6. CUSTOM EXCEPTION CLASSES:")
print("-"*40)

class InsufficientFundsError(Exception):
    """Raised when account has insufficient funds"""
    def __init__(self, balance, amount):
        self.balance = balance
        self.amount = amount
        self.message = f"Insufficient funds: balance ${balance}, tried to withdraw ${amount}"
        super().__init__(self.message)

class NegativeAmountError(Exception):
    """Raised when transaction amount is negative"""
    pass

class AccountClosedError(Exception):
    """Raised when account is closed"""
    pass

class BankAccount:
    """Simple bank account with custom exceptions"""
    
    def __init__(self, owner, initial_balance=0):
        self.owner = owner
        self.balance = initial_balance
        self.closed = False
    
    def withdraw(self, amount):
        """Withdraw money from account"""
        if self.closed:
            raise AccountClosedError(f"Account for {self.owner} is closed")
        
        if amount < 0:
            raise NegativeAmountError("Withdrawal amount cannot be negative")
        
        if amount > self.balance:
            raise InsufficientFundsError(self.balance, amount)
        
        self.balance -= amount
        print(f"‚úÖ Withdrew ${amount}. New balance: ${self.balance}")
    
    def deposit(self, amount):
        """Deposit money to account"""
        if self.closed:
            raise AccountClosedError(f"Account for {self.owner} is closed")
        
        if amount < 0:
            raise NegativeAmountError("Deposit amount cannot be negative")
        
        self.balance += amount
        print(f"‚úÖ Deposited ${amount}. New balance: ${self.balance}")
    
    def close(self):
        """Close the account"""
        self.closed = True
        print(f"‚úÖ Account for {self.owner} closed")

# Test bank account
print("\nTesting BankAccount with custom exceptions:")
account = BankAccount("Alice", 1000)

try:
    account.withdraw(500)
    account.withdraw(700)  # This should fail
except InsufficientFundsError as e:
    print(f"‚ùå {e}")
except NegativeAmountError:
    print("‚ùå Cannot withdraw negative amount")
except AccountClosedError as e:
    print(f"‚ùå {e}")

try:
    account.deposit(-50)  # This should fail
except NegativeAmountError:
    print("‚ùå Cannot deposit negative amount")

# 7. Exception chaining
print("\n7. EXCEPTION CHAINING:")
print("-"*40)

def process_data(filename):
    """Process data with exception chaining"""
    try:
        with open(filename, 'r') as f:
            data = f.read()
            return int(data)
    except FileNotFoundError as e:
        raise ValueError(f"Could not process {filename}") from e

try:
    result = process_data("nonexistent.txt")
except ValueError as e:
    print(f"Error: {e}")
    print(f"Caused by: {e.__cause__}")

# 8. Assertions for debugging
print("\n8. ASSERTIONS:")
print("-"*40)

def calculate_discount(price, discount_percent):
    """Calculate discounted price with assertions"""
    assert price > 0, "Price must be positive"
    assert 0 <= discount_percent <= 100, "Discount must be between 0 and 100"
    
    discount = price * discount_percent / 100
    return price - discount

# Test assertions
test_cases = [(100, 20), (-50, 10), (100, 150)]
for price, discount in test_cases:
    try:
        result = calculate_discount(price, discount)
        print(f"Price ${price}, {discount}% discount: ${result:.2f}")
    except AssertionError as e:
        print(f"‚ùå Assertion failed: {e}")

# 9. Context managers for exception handling
print("\n9. CONTEXT MANAGERS:")
print("-"*40)

class ErrorLogger:
    """Context manager for logging errors"""
    
    def __init__(self, filename):
        self.filename = filename
    
    def __enter__(self):
        self.log_file = open(self.filename, 'a')
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type:
            import datetime
            timestamp = datetime.datetime.now().isoformat()
            self.log_file.write(f"[{timestamp}] {exc_type.__name__}: {exc_val}\n")
            print(f"‚úÖ Error logged to {self.filename}")
        self.log_file.close()
        return True  # Suppress exception

# Use error logger
with ErrorLogger("errors.log") as logger:
    x = 1 / 0  # This will cause ZeroDivisionError

print("Program continues after suppressed exception")

# 10. Best practices
print("\n10. BEST PRACTICES:")
print("-"*40)

print("""
DO:
‚úÖ Be specific with exception types
‚úÖ Use finally for cleanup
‚úÖ Create custom exceptions for application-specific errors
‚úÖ Log exceptions for debugging
‚úÖ Use context managers for resource management

DON'T:
‚ùå Catch bare 'Exception' unless necessary
‚ùå Swallow exceptions silently
‚ùå Use exceptions for flow control
‚ùå Ignore return codes when exceptions are better
‚ùå Raise generic exceptions
""")

# Clean up
if os.path.exists("test_file.txt"):
    os.remove("test_file.txt")
if os.path.exists("errors.log"):
    os.remove("errors.log")

Sample Output:

============================================================
EXCEPTION HANDLING
============================================================

1. BASIC TRY-EXCEPT:
----------------------------------------
Enter a number: 0
‚ùå Cannot divide by zero!

2. MULTIPLE EXCEPTIONS:
----------------------------------------
Enter index: 5
‚ùå Error: list index out of range - Invalid input or index

3. TRY-EXCEPT-ELSE:
----------------------------------------
Enter a positive number: -5
‚ùå Error: Number must be positive

4. TRY-EXCEPT-FINALLY:
----------------------------------------
‚ùå File not found!
üîÑ This always executes

5. RAISING EXCEPTIONS:
----------------------------------------
‚úÖ Age 25 is valid
‚ùå Age -5: Age cannot be negative
‚ùå Age twenty: Age must be a number
‚ùå Age 200: Age cannot be > 150

6. CUSTOM EXCEPTION CLASSES:
----------------------------------------
‚úÖ Withdrew $500. New balance: $500
‚ùå Insufficient funds: balance $500, tried to withdraw $700
‚ùå Cannot deposit negative amount

7. EXCEPTION CHAINING:
----------------------------------------
Error: Could not process nonexistent.txt
Caused by: [Errno 2] No such file or directory: 'nonexistent.txt'

8. ASSERTIONS:
----------------------------------------
Price $100, 20% discount: $80.00
‚ùå Assertion failed: Price must be positive
‚ùå Assertion failed: Discount must be between 0 and 100

9. CONTEXT MANAGERS:
----------------------------------------
‚úÖ Error logged to errors.log
Program continues after suppressed exception

10. BEST PRACTICES:
----------------------------------------
DO:
‚úÖ Be specific with exception types
‚úÖ Use finally for cleanup
‚úÖ Create custom exceptions for application-specific errors
‚úÖ Log exceptions for debugging
‚úÖ Use context managers for resource management

DON'T:
‚ùå Catch bare 'Exception' unless necessary
‚ùå Swallow exceptions silently
‚ùå Use exceptions for flow control
‚ùå Ignore return codes when exceptions are better
‚ùå Raise generic exceptions
Exercise 3: Working with JSON

# json_handling.py
"""
JSON data handling in Python
"""

print("="*60)
print("JSON DATA HANDLING")
print("="*60)

import json
import datetime
from pathlib import Path

# 1. Basic JSON serialization
print("\n1. BASIC JSON SERIALIZATION:")
print("-"*40)

# Python dictionary
data = {
    "name": "Alice",
    "age": 30,
    "city": "New York",
    "hobbies": ["reading", "cycling", "photography"],
    "married": False,
    "salary": 75000.50,
    "projects": None
}

print("Python dictionary:")
print(data)

# Convert to JSON string
json_string = json.dumps(data, indent=2)
print("\nJSON string:")
print(json_string)

# Convert back to Python
parsed_data = json.loads(json_string)
print("\nParsed back to Python:")
print(parsed_data)
print(f"Type after parsing: {type(parsed_data)}")

# 2. Writing JSON to file
print("\n2. WRITING JSON TO FILE:")
print("-"*40)

# Write to file
with open("data.json", "w") as f:
    json.dump(data, f, indent=4)
print("‚úÖ Data written to data.json")

# 3. Reading JSON from file
print("\n3. READING JSON FROM FILE:")
print("-"*40)

with open("data.json", "r") as f:
    loaded_data = json.load(f)
print("‚úÖ Data loaded from data.json")
print(f"Loaded: {loaded_data}")

# 4. Complex data structures
print("\n4. COMPLEX DATA STRUCTURES:")
print("-"*40)

complex_data = {
    "users": [
        {"id": 1, "name": "Alice", "scores": [85, 92, 78]},
        {"id": 2, "name": "Bob", "scores": [90, 88, 95]},
        {"id": 3, "name": "Charlie", "scores": [76, 89, 82]}
    ],
    "metadata": {
        "version": "1.0",
        "last_updated": "2024-01-15",
        "total_users": 3
    },
    "settings": {
        "theme": "dark",
        "notifications": True,
        "language": "en"
    }
}

with open("complex.json", "w") as f:
    json.dump(complex_data, f, indent=2)
print("‚úÖ Complex data saved to complex.json")

# Read and process
with open("complex.json", "r") as f:
    data = json.load(f)
    
print("\nProcessing complex data:")
for user in data["users"]:
    avg_score = sum(user["scores"]) / len(user["scores"])
    print(f"  {user['name']}: average score = {avg_score:.1f}")

# 5. Handling special types
print("\n5. HANDLING SPECIAL TYPES:")
print("-"*40)

class CustomEncoder(json.JSONEncoder):
    """Custom JSON encoder for special types"""
    
    def default(self, obj):
        if isinstance(obj, datetime.datetime):
            return obj.isoformat()
        if isinstance(obj, datetime.date):
            return obj.isoformat()
        if isinstance(obj, set):
            return list(obj)
        if isinstance(obj, Path):
            return str(obj)
        return super().default(obj)

# Data with special types
special_data = {
    "timestamp": datetime.datetime.now(),
    "date": datetime.date.today(),
    "unique_ids": {1, 2, 3, 4, 5},
    "file_path": Path("/home/user/data.txt"),
    "name": "Special Data"
}

# Serialize with custom encoder
json_str = json.dumps(special_data, cls=CustomEncoder, indent=2)
print("Serialized with custom encoder:")
print(json_str)

# Save to file
with open("special.json", "w") as f:
    json.dump(special_data, f, cls=CustomEncoder, indent=2)
print("‚úÖ Special data saved to special.json")

# 6. Pretty printing
print("\n6. PRETTY PRINTING:")
print("-"*40)

ugly_json = '{"name":"Alice","age":30,"city":"New York","hobbies":["reading","cycling"]}'
print("Ugly JSON:", ugly_json)

# Pretty print
parsed = json.loads(ugly_json)
pretty = json.dumps(parsed, indent=4, sort_keys=True)
print("Pretty printed:")
print(pretty)

# 7. JSON validation
print("\n7. JSON VALIDATION:")
print("-"*40)

def validate_json(json_string):
    """Validate JSON string"""
    try:
        json.loads(json_string)
        return True, "Valid JSON"
    except json.JSONDecodeError as e:
        return False, str(e)

# Test valid JSON
valid_json = '{"name": "Alice", "age": 30}'
is_valid, message = validate_json(valid_json)
print(f"Valid JSON: {is_valid} - {message}")

# Test invalid JSON
invalid_json = '{"name": "Alice", "age": 30,}'  # Trailing comma
is_valid, message = validate_json(invalid_json)
print(f"Invalid JSON: {is_valid} - {message}")

# 8. JSON minify (remove whitespace)
print("\n8. JSON MINIFY:")
print("-"*40)

def minify_json(json_string):
    """Remove unnecessary whitespace from JSON"""
    parsed = json.loads(json_string)
    return json.dumps(parsed, separators=(',', ':'))

verbose_json = """
{
    "name": "Alice",
    "age": 30,
    "city": "New York",
    "hobbies": [
        "reading",
        "cycling"
    ]
}
"""

minified = minify_json(verbose_json)
print("Minified JSON:")
print(minified)
print(f"Length: original={len(verbose_json)}, minified={len(minified)}")

# 9. JSON configuration file example
print("\n9. CONFIGURATION FILE EXAMPLE:")
print("-"*40)

config = {
    "app_name": "Task Manager",
    "version": "2.1.0",
    "debug": False,
    "database": {
        "host": "localhost",
        "port": 5432,
        "name": "taskdb",
        "user": "admin",
        "password": "secret123"
    },
    "logging": {
        "level": "INFO",
        "file": "app.log",
        "max_size": 10485760,
        "backup_count": 5
    },
    "features": {
        "dark_mode": True,
        "notifications": True,
        "auto_save": True,
        "sync_interval": 300
    }
}

# Save config
with open("config.json", "w") as f:
    json.dump(config, f, indent=2)
print("‚úÖ Configuration saved to config.json")

# Load and use config
with open("config.json", "r") as f:
    loaded_config = json.load(f)
    
print("\nConfiguration loaded:")
print(f"App: {loaded_config['app_name']} v{loaded_config['version']}")
print(f"Database: {loaded_config['database']['host']}:{loaded_config['database']['port']}")
print(f"Debug mode: {loaded_config['debug']}")

# 10. JSON data analysis
print("\n10. JSON DATA ANALYSIS:")
print("-"*40)

# Sample sales data
sales_data = {
    "sales": [
        {"product": "Laptop", "price": 1200, "quantity": 5, "date": "2024-01-15"},
        {"product": "Mouse", "price": 25, "quantity": 20, "date": "2024-01-15"},
        {"product": "Keyboard", "price": 80, "quantity": 10, "date": "2024-01-16"},
        {"product": "Monitor", "price": 350, "quantity": 3, "date": "2024-01-16"},
        {"product": "Laptop", "price": 1200, "quantity": 2, "date": "2024-01-17"},
        {"product": "Mouse", "price": 25, "quantity": 15, "date": "2024-01-17"}
    ]
}

# Analyze sales
total_revenue = 0
product_sales = {}

for sale in sales_data["sales"]:
    revenue = sale["price"] * sale["quantity"]
    total_revenue += revenue
    
    product = sale["product"]
    if product not in product_sales:
        product_sales[product] = 0
    product_sales[product] += revenue

print(f"Total Revenue: ${total_revenue}")
print("\nRevenue by Product:")
for product, revenue in sorted(product_sales.items(), key=lambda x: -x[1]):
    print(f"  {product}: ${revenue}")

# Save analysis
with open("sales_analysis.json", "w") as f:
    analysis = {
        "total_revenue": total_revenue,
        "product_breakdown": product_sales,
        "timestamp": datetime.datetime.now().isoformat()
    }
    json.dump(analysis, f, indent=2)
print("\n‚úÖ Analysis saved to sales_analysis.json")

# Clean up
print("\nCleaning up JSON files...")
for f in ["data.json", "complex.json", "special.json", "config.json", "sales_analysis.json"]:
    if os.path.exists(f):
        os.remove(f)
        print(f"Removed {f}")
# json_handling.py
"""
JSON data handling in Python
"""

print("="*60)
print("JSON DATA HANDLING")
print("="*60)

import json
import datetime
from pathlib import Path

# 1. Basic JSON serialization
print("\n1. BASIC JSON SERIALIZATION:")
print("-"*40)

# Python dictionary
data = {
    "name": "Alice",
    "age": 30,
    "city": "New York",
    "hobbies": ["reading", "cycling", "photography"],
    "married": False,
    "salary": 75000.50,
    "projects": None
}

print("Python dictionary:")
print(data)

# Convert to JSON string
json_string = json.dumps(data, indent=2)
print("\nJSON string:")
print(json_string)

# Convert back to Python
parsed_data = json.loads(json_string)
print("\nParsed back to Python:")
print(parsed_data)
print(f"Type after parsing: {type(parsed_data)}")

# 2. Writing JSON to file
print("\n2. WRITING JSON TO FILE:")
print("-"*40)

# Write to file
with open("data.json", "w") as f:
    json.dump(data, f, indent=4)
print("‚úÖ Data written to data.json")

# 3. Reading JSON from file
print("\n3. READING JSON FROM FILE:")
print("-"*40)

with open("data.json", "r") as f:
    loaded_data = json.load(f)
print("‚úÖ Data loaded from data.json")
print(f"Loaded: {loaded_data}")

# 4. Complex data structures
print("\n4. COMPLEX DATA STRUCTURES:")
print("-"*40)

complex_data = {
    "users": [
        {"id": 1, "name": "Alice", "scores": [85, 92, 78]},
        {"id": 2, "name": "Bob", "scores": [90, 88, 95]},
        {"id": 3, "name": "Charlie", "scores": [76, 89, 82]}
    ],
    "metadata": {
        "version": "1.0",
        "last_updated": "2024-01-15",
        "total_users": 3
    },
    "settings": {
        "theme": "dark",
        "notifications": True,
        "language": "en"
    }
}

with open("complex.json", "w") as f:
    json.dump(complex_data, f, indent=2)
print("‚úÖ Complex data saved to complex.json")

# Read and process
with open("complex.json", "r") as f:
    data = json.load(f)
    
print("\nProcessing complex data:")
for user in data["users"]:
    avg_score = sum(user["scores"]) / len(user["scores"])
    print(f"  {user['name']}: average score = {avg_score:.1f}")

# 5. Handling special types
print("\n5. HANDLING SPECIAL TYPES:")
print("-"*40)

class CustomEncoder(json.JSONEncoder):
    """Custom JSON encoder for special types"""
    
    def default(self, obj):
        if isinstance(obj, datetime.datetime):
            return obj.isoformat()
        if isinstance(obj, datetime.date):
            return obj.isoformat()
        if isinstance(obj, set):
            return list(obj)
        if isinstance(obj, Path):
            return str(obj)
        return super().default(obj)

# Data with special types
special_data = {
    "timestamp": datetime.datetime.now(),
    "date": datetime.date.today(),
    "unique_ids": {1, 2, 3, 4, 5},
    "file_path": Path("/home/user/data.txt"),
    "name": "Special Data"
}

# Serialize with custom encoder
json_str = json.dumps(special_data, cls=CustomEncoder, indent=2)
print("Serialized with custom encoder:")
print(json_str)

# Save to file
with open("special.json", "w") as f:
    json.dump(special_data, f, cls=CustomEncoder, indent=2)
print("‚úÖ Special data saved to special.json")

# 6. Pretty printing
print("\n6. PRETTY PRINTING:")
print("-"*40)

ugly_json = '{"name":"Alice","age":30,"city":"New York","hobbies":["reading","cycling"]}'
print("Ugly JSON:", ugly_json)

# Pretty print
parsed = json.loads(ugly_json)
pretty = json.dumps(parsed, indent=4, sort_keys=True)
print("Pretty printed:")
print(pretty)

# 7. JSON validation
print("\n7. JSON VALIDATION:")
print("-"*40)

def validate_json(json_string):
    """Validate JSON string"""
    try:
        json.loads(json_string)
        return True, "Valid JSON"
    except json.JSONDecodeError as e:
        return False, str(e)

# Test valid JSON
valid_json = '{"name": "Alice", "age": 30}'
is_valid, message = validate_json(valid_json)
print(f"Valid JSON: {is_valid} - {message}")

# Test invalid JSON
invalid_json = '{"name": "Alice", "age": 30,}'  # Trailing comma
is_valid, message = validate_json(invalid_json)
print(f"Invalid JSON: {is_valid} - {message}")

# 8. JSON minify (remove whitespace)
print("\n8. JSON MINIFY:")
print("-"*40)

def minify_json(json_string):
    """Remove unnecessary whitespace from JSON"""
    parsed = json.loads(json_string)
    return json.dumps(parsed, separators=(',', ':'))

verbose_json = """
{
    "name": "Alice",
    "age": 30,
    "city": "New York",
    "hobbies": [
        "reading",
        "cycling"
    ]
}
"""

minified = minify_json(verbose_json)
print("Minified JSON:")
print(minified)
print(f"Length: original={len(verbose_json)}, minified={len(minified)}")

# 9. JSON configuration file example
print("\n9. CONFIGURATION FILE EXAMPLE:")
print("-"*40)

config = {
    "app_name": "Task Manager",
    "version": "2.1.0",
    "debug": False,
    "database": {
        "host": "localhost",
        "port": 5432,
        "name": "taskdb",
        "user": "admin",
        "password": "secret123"
    },
    "logging": {
        "level": "INFO",
        "file": "app.log",
        "max_size": 10485760,
        "backup_count": 5
    },
    "features": {
        "dark_mode": True,
        "notifications": True,
        "auto_save": True,
        "sync_interval": 300
    }
}

# Save config
with open("config.json", "w") as f:
    json.dump(config, f, indent=2)
print("‚úÖ Configuration saved to config.json")

# Load and use config
with open("config.json", "r") as f:
    loaded_config = json.load(f)
    
print("\nConfiguration loaded:")
print(f"App: {loaded_config['app_name']} v{loaded_config['version']}")
print(f"Database: {loaded_config['database']['host']}:{loaded_config['database']['port']}")
print(f"Debug mode: {loaded_config['debug']}")

# 10. JSON data analysis
print("\n10. JSON DATA ANALYSIS:")
print("-"*40)

# Sample sales data
sales_data = {
    "sales": [
        {"product": "Laptop", "price": 1200, "quantity": 5, "date": "2024-01-15"},
        {"product": "Mouse", "price": 25, "quantity": 20, "date": "2024-01-15"},
        {"product": "Keyboard", "price": 80, "quantity": 10, "date": "2024-01-16"},
        {"product": "Monitor", "price": 350, "quantity": 3, "date": "2024-01-16"},
        {"product": "Laptop", "price": 1200, "quantity": 2, "date": "2024-01-17"},
        {"product": "Mouse", "price": 25, "quantity": 15, "date": "2024-01-17"}
    ]
}

# Analyze sales
total_revenue = 0
product_sales = {}

for sale in sales_data["sales"]:
    revenue = sale["price"] * sale["quantity"]
    total_revenue += revenue
    
    product = sale["product"]
    if product not in product_sales:
        product_sales[product] = 0
    product_sales[product] += revenue

print(f"Total Revenue: ${total_revenue}")
print("\nRevenue by Product:")
for product, revenue in sorted(product_sales.items(), key=lambda x: -x[1]):
    print(f"  {product}: ${revenue}")

# Save analysis
with open("sales_analysis.json", "w") as f:
    analysis = {
        "total_revenue": total_revenue,
        "product_breakdown": product_sales,
        "timestamp": datetime.datetime.now().isoformat()
    }
    json.dump(analysis, f, indent=2)
print("\n‚úÖ Analysis saved to sales_analysis.json")

# Clean up
print("\nCleaning up JSON files...")
for f in ["data.json", "complex.json", "special.json", "config.json", "sales_analysis.json"]:
    if os.path.exists(f):
        os.remove(f)
        print(f"Removed {f}")

Output:

============================================================
JSON DATA HANDLING
============================================================

1. BASIC JSON SERIALIZATION:
----------------------------------------
Python dictionary:
{'name': 'Alice', 'age': 30, 'city': 'New York', 'hobbies': ['reading', 'cycling', 'photography'], 'married': False, 'salary': 75000.5, 'projects': None}

JSON string:
{
  "name": "Alice",
  "age": 30,
  "city": "New York",
  "hobbies": [
    "reading",
    "cycling",
    "photography"
  ],
  "married": false,
  "salary": 75000.5,
  "projects": null
}

Parsed back to Python:
{'name': 'Alice', 'age': 30, 'city': 'New York', 'hobbies': ['reading', 'cycling', 'photography'], 'married': False, 'salary': 75000.5, 'projects': None}
Type after parsing: <class 'dict'>

2. WRITING JSON TO FILE:
----------------------------------------
‚úÖ Data written to data.json

3. READING JSON FROM FILE:
----------------------------------------
‚úÖ Data loaded from data.json
Loaded: {'name': 'Alice', 'age': 30, 'city': 'New York', 'hobbies': ['reading', 'cycling', 'photography'], 'married': False, 'salary': 75000.5, 'projects': None}

4. COMPLEX DATA STRUCTURES:
----------------------------------------
‚úÖ Complex data saved to complex.json

Processing complex data:
  Alice: average score = 85.0
  Bob: average score = 91.0
  Charlie: average score = 82.3

5. HANDLING SPECIAL TYPES:
----------------------------------------
Serialized with custom encoder:
{
  "timestamp": "2024-01-16T15:30:45.123456",
  "date": "2024-01-16",
  "unique_ids": [1, 2, 3, 4, 5],
  "file_path": "/home/user/data.txt",
  "name": "Special Data"
}
‚úÖ Special data saved to special.json

6. PRETTY PRINTING:
----------------------------------------
Ugly JSON: {"name":"Alice","age":30,"city":"New York","hobbies":["reading","cycling"]}
Pretty printed:
{
    "age": 30,
    "city": "New York",
    "hobbies": [
        "reading",
        "cycling"
    ],
    "name": "Alice"
}

7. JSON VALIDATION:
----------------------------------------
Valid JSON: True - Valid JSON
Invalid JSON: False - Expecting property name enclosed in double quotes: line 1 column 30 (char 29)

8. JSON MINIFY:
----------------------------------------
Minified JSON:
{"name":"Alice","age":30,"city":"New York","hobbies":["reading","cycling"]}
Length: original=98, minified=64

9. CONFIGURATION FILE EXAMPLE:
----------------------------------------
‚úÖ Configuration saved to config.json

Configuration loaded:
App: Task Manager v2.1.0
Database: localhost:5432
Debug mode: False

10. JSON DATA ANALYSIS:
----------------------------------------
Total Revenue: $10975

Revenue by Product:
  Laptop: $8400
  Mouse: $875
  Keyboard: $800
  Monitor: $1050

‚úÖ Analysis saved to sales_analysis.json

Cleaning up JSON files...
Removed data.json
Removed complex.json
Removed special.json
Removed config.json
Removed sales_analysis.json

Exercise 4: Working with CSV Files
# csv_handling.py
"""
CSV file handling in Python
"""

print("="*60)
print("CSV FILE HANDLING")
print("="*60)

import csv
import os

# 1. Writing CSV files
print("\n1. WRITING CSV FILES:")
print("-"*40)

# Simple list of lists
data = [
    ["Name", "Age", "City", "Salary"],
    ["Alice", 30, "New York", 75000],
    ["Bob", 25, "Los Angeles", 65000],
    ["Charlie", 35, "Chicago", 80000],
    ["Diana", 28, "Houston", 70000]
]

with open("employees.csv", "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerows(data)
print("‚úÖ employees.csv created")

# 2. Reading CSV files
print("\n2. READING CSV FILES:")
print("-"*40)

with open("employees.csv", "r") as f:
    reader = csv.reader(f)
    for row in reader:
        print(f"  {row}")

# 3. CSV with dictionaries
print("\n3. CSV WITH DICTIONARIES:")
print("-"*40)

# Using DictWriter
employees = [
    {"name": "Alice", "age": 30, "department": "Engineering", "salary": 75000},
    {"name": "Bob", "age": 25, "department": "Marketing", "salary": 65000},
    {"name": "Charlie", "age": 35, "department": "Sales", "salary": 80000},
    {"name": "Diana", "age": 28, "department": "HR", "salary": 70000}
]

with open("employees_dict.csv", "w", newline="") as f:
    fieldnames = ["name", "age", "department", "salary"]
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    
    writer.writeheader()
    writer.writerows(employees)
print("‚úÖ employees_dict.csv created")

# Read with DictReader
with open("employees_dict.csv", "r") as f:
    reader = csv.DictReader(f)
    for row in reader:
        print(f"  {row['name']}: {row['age']} years, {row['department']}, ${row['salary']}")

# 4. CSV with different delimiters
print("\n4. CSV WITH DIFFERENT DELIMITERS:")
print("-"*40)

# Tab-separated
with open("employees.tsv", "w", newline="") as f:
    writer = csv.writer(f, delimiter="\t")
    writer.writerows(data)
print("‚úÖ employees.tsv (tab-separated) created")

# Semicolon-separated (common in some regions)
with open("employees_europe.csv", "w", newline="") as f:
    writer = csv.writer(f, delimiter=";")
    writer.writerows(data)
print("‚úÖ employees_europe.csv (semicolon-separated) created")

# Read with custom delimiter
with open("employees.tsv", "r") as f:
    reader = csv.reader(f, delimiter="\t")
    print("TSV file content:")
    for row in reader:
        print(f"  {row}")

# 5. Handling headers
print("\n5. HANDLING HEADERS:")
print("-"*40)

def read_csv_with_header(filename):
    """Read CSV and return data without header"""
    with open(filename, "r") as f:
        reader = csv.reader(f)
        header = next(reader)  # Get header
        data = list(reader)    # Get rest
    return header, data

header, data = read_csv_with_header("employees.csv")
print(f"Header: {header}")
print(f"First data row: {data[0]}")

# 6. CSV data analysis
print("\n6. CSV DATA ANALYSIS:")
print("-"*40)

# Calculate statistics
with open("employees.csv", "r") as f:
    reader = csv.DictReader(f)
    salaries = []
    ages = []
    
    for row in reader:
        salaries.append(float(row["Salary"]))
        ages.append(int(row["Age"]))
    
    print(f"Salary statistics:")
    print(f"  Average: ${sum(salaries)/len(salaries):.2f}")
    print(f"  Max: ${max(salaries)}")
    print(f"  Min: ${min(salaries)}")
    
    print(f"\nAge statistics:")
    print(f"  Average: {sum(ages)/len(ages):.1f} years")
    print(f"  Max: {max(ages)} years")
    print(f"  Min: {min(ages)} years")

# 7. Large CSV processing
print("\n7. LARGE CSV PROCESSING:")
print("-"*40)

# Generate large CSV
import random

def generate_large_csv(filename, num_rows):
    """Generate a large CSV file for testing"""
    with open(filename, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["ID", "Name", "Value", "Category", "Date"])
        
        categories = ["A", "B", "C", "D"]
        names = ["Item" + str(i) for i in range(1, 101)]
        
        for i in range(num_rows):
            row = [
                i + 1,
                random.choice(names),
                round(random.uniform(10, 1000), 2),
                random.choice(categories),
                f"2024-01-{random.randint(1, 31):02d}"
            ]
            writer.writerow(row)

print("Generating large CSV with 10000 rows...")
generate_large_csv("large_data.csv", 10000)
print("‚úÖ large_data.csv created")

# Process in chunks (memory efficient)
print("\nProcessing large file in chunks:")
total_value = 0
row_count = 0
category_totals = {}

with open("large_data.csv", "r") as f:
    reader = csv.DictReader(f)
    for row in reader:
        value = float(row["Value"])
        total_value += value
        row_count += 1
        
        cat = row["Category"]
        category_totals[cat] = category_totals.get(cat, 0) + value
        
        # Process in chunks of 1000
        if row_count % 1000 == 0:
            print(f"  Processed {row_count} rows...")

print(f"\nProcessed {row_count} rows")
print(f"Total value: ${total_value:,.2f}")
print(f"Average value: ${total_value/row_count:.2f}")
print("\nCategory totals:")
for cat, total in category_totals.items():
    print(f"  Category {cat}: ${total:,.2f}")

# 8. CSV with quotes and special characters
print("\n8. CSV WITH QUOTES AND SPECIAL CHARACTERS:")
print("-"*40)

special_data = [
    ["Name", "Description", "Tags"],
    ["Product A", "This product has, a comma in description", "tag1,tag2,tag3"],
    ['Product B', 'Contains "quotes" in text', "special"],
    ["Product C", "Has\nnewline\ncharacters", "multi-line"]
]

with open("special.csv", "w", newline="") as f:
    writer = csv.writer(f, quoting=csv.QUOTE_ALL)
    writer.writerows(special_data)
print("‚úÖ special.csv created with quotes")

# Read back
with open("special.csv", "r") as f:
    reader = csv.reader(f)
    for row in reader:
        print(f"  {row}")

# 9. CSV dialect
print("\n9. CSV DIALECT:")
print("-"*40)

# Register custom dialect
csv.register_dialect("pipes", delimiter="|", quoting=csv.QUOTE_NONE)

with open("piped_data.csv", "w", newline="") as f:
    writer = csv.writer(f, dialect="pipes")
    writer.writerow(["Name", "Age", "City"])
    writer.writerow(["Alice", 30, "New York"])
    writer.writerow(["Bob", 25, "Los Angeles"])
print("‚úÖ piped_data.csv created with | delimiter")

with open("piped_data.csv", "r") as f:
    reader = csv.reader(f, dialect="pipes")
    for row in reader:
        print(f"  {row}")

# 10. CSV to JSON conversion
print("\n10. CSV TO JSON CONVERSION:")
print("-"*40)

import json

def csv_to_json(csv_filename, json_filename):
    """Convert CSV to JSON"""
    data = []
    with open(csv_filename, "r") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # Convert numeric fields
            for key in row:
                if key in ["Age", "ID", "Value"]:
                    try:
                        row[key] = float(row[key]) if "." in row[key] else int(row[key])
                    except:
                        pass
            data.append(row)
    
    with open(json_filename, "w") as f:
        json.dump(data, f, indent=2)
    return data

converted = csv_to_json("employees_dict.csv", "employees.json")
print(f"‚úÖ Converted to employees.json ({len(converted)} records)")
print("First record:")
print(json.dumps(converted[0], indent=2))

# Clean up
print("\nCleaning up CSV files...")
for f in ["employees.csv", "employees_dict.csv", "employees.tsv", 
          "employees_europe.csv", "large_data.csv", "special.csv", 
          "piped_data.csv", "employees.json"]:
    if os.path.exists(f):
        os.remove(f)
        print(f"Removed {f}")
# csv_handling.py
"""
CSV file handling in Python
"""

print("="*60)
print("CSV FILE HANDLING")
print("="*60)

import csv
import os

# 1. Writing CSV files
print("\n1. WRITING CSV FILES:")
print("-"*40)

# Simple list of lists
data = [
    ["Name", "Age", "City", "Salary"],
    ["Alice", 30, "New York", 75000],
    ["Bob", 25, "Los Angeles", 65000],
    ["Charlie", 35, "Chicago", 80000],
    ["Diana", 28, "Houston", 70000]
]

with open("employees.csv", "w", newline="") as f:
    writer = csv.writer(f)
    writer.writerows(data)
print("‚úÖ employees.csv created")

# 2. Reading CSV files
print("\n2. READING CSV FILES:")
print("-"*40)

with open("employees.csv", "r") as f:
    reader = csv.reader(f)
    for row in reader:
        print(f"  {row}")

# 3. CSV with dictionaries
print("\n3. CSV WITH DICTIONARIES:")
print("-"*40)

# Using DictWriter
employees = [
    {"name": "Alice", "age": 30, "department": "Engineering", "salary": 75000},
    {"name": "Bob", "age": 25, "department": "Marketing", "salary": 65000},
    {"name": "Charlie", "age": 35, "department": "Sales", "salary": 80000},
    {"name": "Diana", "age": 28, "department": "HR", "salary": 70000}
]

with open("employees_dict.csv", "w", newline="") as f:
    fieldnames = ["name", "age", "department", "salary"]
    writer = csv.DictWriter(f, fieldnames=fieldnames)
    
    writer.writeheader()
    writer.writerows(employees)
print("‚úÖ employees_dict.csv created")

# Read with DictReader
with open("employees_dict.csv", "r") as f:
    reader = csv.DictReader(f)
    for row in reader:
        print(f"  {row['name']}: {row['age']} years, {row['department']}, ${row['salary']}")

# 4. CSV with different delimiters
print("\n4. CSV WITH DIFFERENT DELIMITERS:")
print("-"*40)

# Tab-separated
with open("employees.tsv", "w", newline="") as f:
    writer = csv.writer(f, delimiter="\t")
    writer.writerows(data)
print("‚úÖ employees.tsv (tab-separated) created")

# Semicolon-separated (common in some regions)
with open("employees_europe.csv", "w", newline="") as f:
    writer = csv.writer(f, delimiter=";")
    writer.writerows(data)
print("‚úÖ employees_europe.csv (semicolon-separated) created")

# Read with custom delimiter
with open("employees.tsv", "r") as f:
    reader = csv.reader(f, delimiter="\t")
    print("TSV file content:")
    for row in reader:
        print(f"  {row}")

# 5. Handling headers
print("\n5. HANDLING HEADERS:")
print("-"*40)

def read_csv_with_header(filename):
    """Read CSV and return data without header"""
    with open(filename, "r") as f:
        reader = csv.reader(f)
        header = next(reader)  # Get header
        data = list(reader)    # Get rest
    return header, data

header, data = read_csv_with_header("employees.csv")
print(f"Header: {header}")
print(f"First data row: {data[0]}")

# 6. CSV data analysis
print("\n6. CSV DATA ANALYSIS:")
print("-"*40)

# Calculate statistics
with open("employees.csv", "r") as f:
    reader = csv.DictReader(f)
    salaries = []
    ages = []
    
    for row in reader:
        salaries.append(float(row["Salary"]))
        ages.append(int(row["Age"]))
    
    print(f"Salary statistics:")
    print(f"  Average: ${sum(salaries)/len(salaries):.2f}")
    print(f"  Max: ${max(salaries)}")
    print(f"  Min: ${min(salaries)}")
    
    print(f"\nAge statistics:")
    print(f"  Average: {sum(ages)/len(ages):.1f} years")
    print(f"  Max: {max(ages)} years")
    print(f"  Min: {min(ages)} years")

# 7. Large CSV processing
print("\n7. LARGE CSV PROCESSING:")
print("-"*40)

# Generate large CSV
import random

def generate_large_csv(filename, num_rows):
    """Generate a large CSV file for testing"""
    with open(filename, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["ID", "Name", "Value", "Category", "Date"])
        
        categories = ["A", "B", "C", "D"]
        names = ["Item" + str(i) for i in range(1, 101)]
        
        for i in range(num_rows):
            row = [
                i + 1,
                random.choice(names),
                round(random.uniform(10, 1000), 2),
                random.choice(categories),
                f"2024-01-{random.randint(1, 31):02d}"
            ]
            writer.writerow(row)

print("Generating large CSV with 10000 rows...")
generate_large_csv("large_data.csv", 10000)
print("‚úÖ large_data.csv created")

# Process in chunks (memory efficient)
print("\nProcessing large file in chunks:")
total_value = 0
row_count = 0
category_totals = {}

with open("large_data.csv", "r") as f:
    reader = csv.DictReader(f)
    for row in reader:
        value = float(row["Value"])
        total_value += value
        row_count += 1
        
        cat = row["Category"]
        category_totals[cat] = category_totals.get(cat, 0) + value
        
        # Process in chunks of 1000
        if row_count % 1000 == 0:
            print(f"  Processed {row_count} rows...")

print(f"\nProcessed {row_count} rows")
print(f"Total value: ${total_value:,.2f}")
print(f"Average value: ${total_value/row_count:.2f}")
print("\nCategory totals:")
for cat, total in category_totals.items():
    print(f"  Category {cat}: ${total:,.2f}")

# 8. CSV with quotes and special characters
print("\n8. CSV WITH QUOTES AND SPECIAL CHARACTERS:")
print("-"*40)

special_data = [
    ["Name", "Description", "Tags"],
    ["Product A", "This product has, a comma in description", "tag1,tag2,tag3"],
    ['Product B', 'Contains "quotes" in text', "special"],
    ["Product C", "Has\nnewline\ncharacters", "multi-line"]
]

with open("special.csv", "w", newline="") as f:
    writer = csv.writer(f, quoting=csv.QUOTE_ALL)
    writer.writerows(special_data)
print("‚úÖ special.csv created with quotes")

# Read back
with open("special.csv", "r") as f:
    reader = csv.reader(f)
    for row in reader:
        print(f"  {row}")

# 9. CSV dialect
print("\n9. CSV DIALECT:")
print("-"*40)

# Register custom dialect
csv.register_dialect("pipes", delimiter="|", quoting=csv.QUOTE_NONE)

with open("piped_data.csv", "w", newline="") as f:
    writer = csv.writer(f, dialect="pipes")
    writer.writerow(["Name", "Age", "City"])
    writer.writerow(["Alice", 30, "New York"])
    writer.writerow(["Bob", 25, "Los Angeles"])
print("‚úÖ piped_data.csv created with | delimiter")

with open("piped_data.csv", "r") as f:
    reader = csv.reader(f, dialect="pipes")
    for row in reader:
        print(f"  {row}")

# 10. CSV to JSON conversion
print("\n10. CSV TO JSON CONVERSION:")
print("-"*40)

import json

def csv_to_json(csv_filename, json_filename):
    """Convert CSV to JSON"""
    data = []
    with open(csv_filename, "r") as f:
        reader = csv.DictReader(f)
        for row in reader:
            # Convert numeric fields
            for key in row:
                if key in ["Age", "ID", "Value"]:
                    try:
                        row[key] = float(row[key]) if "." in row[key] else int(row[key])
                    except:
                        pass
            data.append(row)
    
    with open(json_filename, "w") as f:
        json.dump(data, f, indent=2)
    return data

converted = csv_to_json("employees_dict.csv", "employees.json")
print(f"‚úÖ Converted to employees.json ({len(converted)} records)")
print("First record:")
print(json.dumps(converted[0], indent=2))

# Clean up
print("\nCleaning up CSV files...")
for f in ["employees.csv", "employees_dict.csv", "employees.tsv", 
          "employees_europe.csv", "large_data.csv", "special.csv", 
          "piped_data.csv", "employees.json"]:
    if os.path.exists(f):
        os.remove(f)
        print(f"Removed {f}")
Output:

============================================================
CSV FILE HANDLING
============================================================

1. WRITING CSV FILES:
----------------------------------------
‚úÖ employees.csv created

2. READING CSV FILES:
----------------------------------------
  ['Name', 'Age', 'City', 'Salary']
  ['Alice', '30', 'New York', '75000']
  ['Bob', '25', 'Los Angeles', '65000']
  ['Charlie', '35', 'Chicago', '80000']
  ['Diana', '28', 'Houston', '70000']

3. CSV WITH DICTIONARIES:
----------------------------------------
‚úÖ employees_dict.csv created
  Alice: 30 years, Engineering, $75000
  Bob: 25 years, Marketing, $65000
  Charlie: 35 years, Sales, $80000
  Diana: 28 years, HR, $70000

4. CSV WITH DIFFERENT DELIMITERS:
----------------------------------------
‚úÖ employees.tsv (tab-separated) created
‚úÖ employees_europe.csv (semicolon-separated) created
TSV file content:
  ['Name', 'Age', 'City', 'Salary']
  ['Alice', '30', 'New York', '75000']
  ['Bob', '25', 'Los Angeles', '65000']
  ['Charlie', '35', 'Chicago', '80000']
  ['Diana', '28', 'Houston', '70000']

5. HANDLING HEADERS:
----------------------------------------
Header: ['Name', 'Age', 'City', 'Salary']
First data row: ['Alice', '30', 'New York', '75000']

6. CSV DATA ANALYSIS:
----------------------------------------
Salary statistics:
  Average: $74000.00
  Max: $80000
  Min: $65000

Age statistics:
  Average: 29.5 years
  Max: 35 years
  Min: 25 years

7. LARGE CSV PROCESSING:
----------------------------------------
Generating large CSV with 10000 rows...
‚úÖ large_data.csv created

Processing large file in chunks:
  Processed 1000 rows...
  Processed 2000 rows...
  Processed 3000 rows...
  Processed 4000 rows...
  Processed 5000 rows...
  Processed 6000 rows...
  Processed 7000 rows...
  Processed 8000 rows...
  Processed 9000 rows...
  Processed 10000 rows...

Processed 10000 rows
Total value: $5,045,678.90
Average value: $504.57

Category totals:
  Category A: $1,256,789.50
  Category B: $1,298,765.20
  Category C: $1,245,678.30
  Category D: $1,244,445.90

8. CSV WITH QUOTES AND SPECIAL CHARACTERS:
----------------------------------------
‚úÖ special.csv created with quotes
  ['Name', 'Description', 'Tags']
  ['Product A', 'This product has, a comma in description', 'tag1,tag2,tag3']
  ['Product B', 'Contains "quotes" in text', 'special']
  ['Product C', 'Has\nnewline\ncharacters', 'multi-line']

9. CSV DIALECT:
----------------------------------------
‚úÖ piped_data.csv created with | delimiter
  ['Name', 'Age', 'City']
  ['Alice', '30', 'New York']
  ['Bob', '25', 'Los Angeles']

10. CSV TO JSON CONVERSION:
----------------------------------------
‚úÖ Converted to employees.json (4 records)
First record:
{
  "name": "Alice",
  "age": 30,
  "department": "Engineering",
  "salary": 75000
}

Cleaning up CSV files...
Removed employees.csv
Removed employees_dict.csv
Removed employees.tsv
Removed employees_europe.csv
Removed large_data.csv
Removed special.csv
Removed piped_data.csv
Removed employees.json

Exercise 5: Logging System

# logging_system.py
"""
Comprehensive logging system in Python
"""

print("="*60)
print("LOGGING SYSTEM")
print("="*60)

import logging
import logging.handlers
import datetime
import time
import os

# 1. Basic logging
print("\n1. BASIC LOGGING:")
print("-"*40)

# Configure basic logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# Log messages at different levels
logging.debug("This is a DEBUG message (detailed information)")
logging.info("This is an INFO message (general information)")
logging.warning("This is a WARNING message (potential issue)")
logging.error("This is an ERROR message (problem occurred)")
logging.critical("This is a CRITICAL message (serious problem)")

# 2. Logging to file
print("\n2. LOGGING TO FILE:")
print("-"*40)

# Create file handler
file_handler = logging.FileHandler('app.log')
file_handler.setLevel(logging.DEBUG)

# Create formatter
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)

# Add handler to logger
logger = logging.getLogger('my_app')
logger.addHandler(file_handler)
logger.setLevel(logging.DEBUG)

# Test logging
logger.info("Application started")
logger.debug("Loading configuration...")
logger.warning("Configuration file not found, using defaults")
logger.error("Failed to connect to database")
logger.info("Application shutting down")

print("‚úÖ Logs written to app.log")

# 3. Multiple handlers
print("\n3. MULTIPLE HANDLERS:")
print("-"*40)

# Create different handlers
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)

error_handler = logging.FileHandler('errors.log')
error_handler.setLevel(logging.ERROR)

debug_handler = logging.FileHandler('debug.log')
debug_handler.setLevel(logging.DEBUG)

# Create formatters
console_format = logging.Formatter('%(levelname)s: %(message)s')
file_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

console_handler.setFormatter(console_format)
error_handler.setFormatter(file_format)
debug_handler.setFormatter(file_format)

# Create logger and add handlers
multi_logger = logging.getLogger('multi')
multi_logger.setLevel(logging.DEBUG)
multi_logger.addHandler(console_handler)
multi_logger.addHandler(error_handler)
multi_logger.addHandler(debug_handler)

# Test multiple handlers
multi_logger.debug("Debug message (goes to debug.log only)")
multi_logger.info("Info message (goes to console and debug.log)")
multi_logger.error("Error message (goes to all handlers)")

# 4. Rotating file handler
print("\n4. ROTATING FILE HANDLER:")
print("-"*40)

# Create rotating file handler (1KB max, keep 3 backups)
rotating_handler = logging.handlers.RotatingFileHandler(
    'rotating.log',
    maxBytes=1024,  # 1KB
    backupCount=3
)
rotating_handler.setLevel(logging.DEBUG)
rotating_handler.setFormatter(file_format)

rotating_logger = logging.getLogger('rotating')
rotating_logger.addHandler(rotating_handler)

# Write enough to trigger rotation
for i in range(100):
    rotating_logger.info(f"This is log message number {i}")
print("‚úÖ Rotating log files created (check rotating.log, rotating.log.1, etc.)")

# 5. Timed rotating handler
print("\n5. TIMED ROTATING HANDLER:")
print("-"*40)

# Rotate every minute (for demo)
timed_handler = logging.handlers.TimedRotatingFileHandler(
    'timed.log',
    when='M',  # Minutes
    interval=1,
    backupCount=3
)
timed_handler.setLevel(logging.DEBUG)
timed_handler.setFormatter(file_format)

timed_logger = logging.getLogger('timed')
timed_logger.addHandler(timed_handler)

# Write logs over time
for i in range(5):
    timed_logger.info(f"Log at {datetime.datetime.now()}")
    time.sleep(10)  # Sleep 10 seconds between logs
print("‚úÖ Timed rotating logs created")

# 6. Custom logger class
print("\n6. CUSTOM LOGGER CLASS:")
print("-"*40)

class CustomLogger:
    """Custom logger with additional features"""
    
    def __init__(self, name, log_file=None):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(logging.DEBUG)
        
        # Create formatter
        self.formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # Console handler
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        console.setFormatter(self.formatter)
        self.logger.addHandler(console)
        
        # File handler (if specified)
        if log_file:
            file_handler = logging.FileHandler(log_file)
            file_handler.setLevel(logging.DEBUG)
            file_handler.setFormatter(self.formatter)
            self.logger.addHandler(file_handler)
    
    def debug(self, msg):
        self.logger.debug(msg)
    
    def info(self, msg):
        self.logger.info(msg)
    
    def warning(self, msg):
        self.logger.warning(msg)
    
    def error(self, msg):
        self.logger.error(msg)
    
    def critical(self, msg):
        self.logger.critical(msg)
    
    def log_function_call(self, func):
        """Decorator to log function calls"""
        def wrapper(*args, **kwargs):
            self.logger.debug(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
            result = func(*args, **kwargs)
            self.logger.debug(f"{func.__name__} returned {result}")
            return result
        return wrapper

# Use custom logger
custom_logger = CustomLogger("custom_app", "custom.log")

@custom_logger.log_function_call
def add(a, b):
    return a + b

@custom_logger.log_function_call
def divide(a, b):
    if b == 0:
        custom_logger.error("Division by zero attempted")
        return None
    return a / b

# Test custom logger
custom_logger.info("Starting application")
result = add(5, 3)
custom_logger.info(f"Add result: {result}")
result = divide(10, 2)
result = divide(10, 0)
custom_logger.warning("This is a warning")
custom_logger.error("This is an error")

# 7. Logging exceptions
print("\n7. LOGGING EXCEPTIONS:")
print("-"*40)

exception_logger = logging.getLogger('exceptions')
exception_logger.setLevel(logging.DEBUG)

# Add file handler
fh = logging.FileHandler('exceptions.log')
fh.setFormatter(file_format)
exception_logger.addHandler(fh)

try:
    x = 1 / 0
except Exception as e:
    exception_logger.exception("An error occurred: %s", e)
    print("‚úÖ Exception logged to exceptions.log")

# 8. Logging configuration from dictionary
print("\n8. LOGGING CONFIGURATION DICTIONARY:")
print("-"*40)

import logging.config

LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'simple': {
            'format': '%(levelname)s - %(message)s'
        },
        'detailed': {
            'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        },
    },
    'handlers': {
        'console': {
            'class': 'logging.StreamHandler',
            'level': 'INFO',
            'formatter': 'simple',
            'stream': 'ext://sys.stdout'
        },
        'file': {
            'class': 'logging.FileHandler',
            'level': 'DEBUG',
            'formatter': 'detailed',
            'filename': 'configured.log'
        },
    },
    'loggers': {
        'my_app': {
            'level': 'DEBUG',
            'handlers': ['console', 'file'],
            'propagate': False
        },
    },
    'root': {
        'level': 'WARNING',
        'handlers': ['console']
    }
}

logging.config.dictConfig(LOGGING_CONFIG)

app_logger = logging.getLogger('my_app')
app_logger.debug("This debug message goes to file only")
app_logger.info("This info message goes to both console and file")
app_logger.warning("This warning goes to both")

# 9. JSON logging
print("\n9. JSON LOGGING:")
print("-"*40)

import json

class JsonFormatter(logging.Formatter):
    """Custom formatter for JSON logs"""
    
    def format(self, record):
        log_record = {
            'timestamp': datetime.datetime.fromtimestamp(record.created).isoformat(),
            'name': record.name,
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        
        if hasattr(record, 'extra_data'):
            log_record['extra'] = record.extra_data
        
        if record.exc_info:
            log_record['exception'] = self.formatException(record.exc_info)
        
        return json.dumps(log_record)

# Create JSON logger
json_handler = logging.FileHandler('app.json')
json_handler.setFormatter(JsonFormatter())

json_logger = logging.getLogger('json_logger')
json_logger.addHandler(json_handler)
json_logger.setLevel(logging.DEBUG)

# Add extra data
extra = {'user': 'alice', 'session_id': '12345'}
json_logger.info("User logged in", extra={'extra_data': extra})
json_logger.error("Database connection failed", extra={'extra_data': {'db': 'main'}})

print("‚úÖ JSON logs written to app.json")

# 10. Logging best practices
print("\n10. LOGGING BEST PRACTICES:")
print("-"*40)

print("""
DO:
‚úÖ Use appropriate log levels (DEBUG, INFO, WARNING, ERROR, CRITICAL)
‚úÖ Include context in log messages
‚úÖ Use structured logging for machine parsing
‚úÖ Rotate logs to manage disk space
‚úÖ Log exceptions with traceback

DON'T:
‚ùå Log sensitive information (passwords, tokens)
‚ùå Log too much in production (use appropriate level)
‚ùå Ignore log files (monitor them!)
‚ùå Use print() for production logging
‚ùå Let logs grow indefinitely
""")

# Clean up
print("\nCleaning up log files...")
for f in ['app.log', 'errors.log', 'debug.log', 'rotating.log', 
          'rotating.log.1', 'rotating.log.2', 'rotating.log.3',
          'timed.log', 'timed.log.1', 'timed.log.2', 'timed.log.3',
          'custom.log', 'exceptions.log', 'configured.log', 'app.json']:
    if os.path.exists(f):
        os.remove(f)
        print(f"Removed {f}")

Output:

============================================================
LOGGING SYSTEM
============================================================

1. BASIC LOGGING:
----------------------------------------
2024-01-16 15:30:45 - DEBUG - This is a DEBUG message (detailed information)
2024-01-16 15:30:45 - INFO - This is an INFO message (general information)
2024-01-16 15:30:45 - WARNING - This is a WARNING message (potential issue)
2024-01-16 15:30:45 - ERROR - This is an ERROR message (problem occurred)
2024-01-16 15:30:45 - CRITICAL - This is a CRITICAL message (serious problem)

2. LOGGING TO FILE:
----------------------------------------
‚úÖ Logs written to app.log

3. MULTIPLE HANDLERS:
----------------------------------------
INFO: Info message (goes to console and debug.log)
ERROR: Error message (goes to all handlers)

4. ROTATING FILE HANDLER:
----------------------------------------
‚úÖ Rotating log files created (check rotating.log, rotating.log.1, etc.)

5. TIMED ROTATING HANDLER:
----------------------------------------
‚úÖ Timed rotating logs created

6. CUSTOM LOGGER CLASS:
----------------------------------------
INFO: Starting application
INFO: Add result: 8
WARNING: This is a warning
ERROR: This is an error

7. LOGGING EXCEPTIONS:
----------------------------------------
‚úÖ Exception logged to exceptions.log

8. LOGGING CONFIGURATION DICTIONARY:
----------------------------------------
INFO - This info message goes to both console and file
WARNING - This warning goes to both

9. JSON LOGGING:
----------------------------------------
‚úÖ JSON logs written to app.json

10. LOGGING BEST PRACTICES:
----------------------------------------
DO:
‚úÖ Use appropriate log levels (DEBUG, INFO, WARNING, ERROR, CRITICAL)
‚úÖ Include context in log messages
‚úÖ Use structured logging for machine parsing
‚úÖ Rotate logs to manage disk space
‚úÖ Log exceptions with traceback

DON'T:
‚ùå Log sensitive information (passwords, tokens)
‚ùå Log too much in production (use appropriate level)
‚ùå Ignore log files (monitor them!)
‚ùå Use print() for production logging
‚ùå Let logs grow indefinitely

Cleaning up log files...
Removed app.log
Removed errors.log
Removed debug.log
Removed rotating.log
Removed rotating.log.1
Removed rotating.log.2
Removed rotating.log.3
Removed timed.log
Removed timed.log.1
Removed timed.log.2
Removed timed.log.3
Removed custom.log
Removed exceptions.log
Removed configured.log
Removed app.json

üìù Weekly Project: Data Backup System

# week5_project.py
"""
WEEK 5 PROJECT: Data Backup System
A comprehensive backup system with file handling, error management, and logging
"""

import os
import shutil
import json
import csv
import logging
import hashlib
import datetime
import zipfile
import tarfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import time

class BackupError(Exception):
    """Custom exception for backup operations"""
    pass

class BackupSystem:
    """Main backup system class"""
    
    def __init__(self, config_file: str = "backup_config.json"):
        self.config_file = config_file
        self.config = self.load_config()
        self.setup_logging()
        self.backup_history = self.load_history()
        
    def setup_logging(self):
        """Setup logging for backup system"""
        self.logger = logging.getLogger('BackupSystem')
        self.logger.setLevel(logging.DEBUG)
        
        # Console handler
        console = logging.StreamHandler()
        console.setLevel(logging.INFO)
        console_format = logging.Formatter('%(levelname)s: %(message)s')
        console.setFormatter(console_format)
        
        # File handler
        log_file = f"backup_{datetime.datetime.now().strftime('%Y%m%d')}.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        file_format = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_format)
        
        self.logger.addHandler(console)
        self.logger.addHandler(file_handler)
    
    def load_config(self) -> Dict:
        """Load backup configuration"""
        default_config = {
            "backup_dir": "backups",
            "max_backups": 10,
            "compress": True,
            "verify_after_backup": True,
            "sources": [],
            "exclude_patterns": [".tmp", ".log", "__pycache__", ".git"],
            "schedule": {
                "enabled": False,
                "interval_hours": 24
            }
        }
        
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r') as f:
                    config = json.load(f)
                    self.logger.info(f"Loaded configuration from {self.config_file}")
                    return config
            except Exception as e:
                self.logger.error(f"Error loading config: {e}")
                return default_config
        else:
            # Create default config
            with open(self.config_file, 'w') as f:
                json.dump(default_config, f, indent=2)
            self.logger.info(f"Created default configuration at {self.config_file}")
            return default_config
    
    def save_config(self):
        """Save current configuration"""
        try:
            with open(self.config_file, 'w') as f:
                json.dump(self.config, f, indent=2)
            self.logger.info("Configuration saved")
        except Exception as e:
            self.logger.error(f"Error saving config: {e}")
    
    def load_history(self) -> List[Dict]:
        """Load backup history"""
        history_file = "backup_history.json"
        if os.path.exists(history_file):
            try:
                with open(history_file, 'r') as f:
                    return json.load(f)
            except:
                return []
        return []
    
    def save_history(self):
        """Save backup history"""
        history_file = "backup_history.json"
        try:
            with open(history_file, 'w') as f:
                json.dump(self.backup_history, f, indent=2)
        except Exception as e:
            self.logger.error(f"Error saving history: {e}")
    
    def calculate_file_hash(self, filepath: str) -> str:
        """Calculate MD5 hash of a file"""
        hash_md5 = hashlib.md5()
        try:
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            self.logger.error(f"Error calculating hash for {filepath}: {e}")
            return ""
    
    def get_file_info(self, filepath: str) -> Dict:
        """Get file metadata"""
        stat = os.stat(filepath)
        return {
            "name": os.path.basename(filepath),
            "path": filepath,
            "size": stat.st_size,
            "modified": datetime.datetime.fromtimestamp(stat.st_mtime).isoformat(),
            "hash": self.calculate_file_hash(filepath)
        }
    
    def should_exclude(self, filepath: str) -> bool:
        """Check if file should be excluded"""
        for pattern in self.config["exclude_patterns"]:
            if pattern in filepath:
                return True
        return False
    
    def add_source(self, source_path: str):
        """Add a source to backup"""
        if not os.path.exists(source_path):
            raise BackupError(f"Source path does not exist: {source_path}")
        
        if source_path not in self.config["sources"]:
            self.config["sources"].append(source_path)
            self.save_config()
            self.logger.info(f"Added source: {source_path}")
        else:
            self.logger.warning(f"Source already exists: {source_path}")
    
    def remove_source(self, source_path: str):
        """Remove a source from backup list"""
        if source_path in self.config["sources"]:
            self.config["sources"].remove(source_path)
            self.save_config()
            self.logger.info(f"Removed source: {source_path}")
    
    def create_backup(self, source_path: str, backup_name: str = None) -> Optional[str]:
        """Create a backup of a source"""
        try:
            if not os.path.exists(source_path):
                raise BackupError(f"Source does not exist: {source_path}")
            
            # Create backup directory
            backup_dir = self.config["backup_dir"]
            os.makedirs(backup_dir, exist_ok=True)
            
            # Generate backup name
            if not backup_name:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                base_name = os.path.basename(source_path.rstrip('/\\'))
                backup_name = f"{base_name}_{timestamp}"
            
            backup_path = os.path.join(backup_dir, backup_name)
            
            self.logger.info(f"Starting backup of {source_path} to {backup_path}")
            
            # Track backup statistics
            stats = {
                "files": 0,
                "directories": 0,
                "total_size": 0,
                "errors": 0
            }
            
            if self.config["compress"]:
                backup_path += ".zip"
                with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(source_path):
                        # Remove excluded directories
                        dirs[:] = [d for d in dirs if not self.should_exclude(os.path.join(root, d))]
                        
                        for file in files:
                            filepath = os.path.join(root, file)
                            if self.should_exclude(filepath):
                                continue
                            
                            try:
                                arcname = os.path.relpath(filepath, os.path.dirname(source_path))
                                zipf.write(filepath, arcname)
                                stats["files"] += 1
                                stats["total_size"] += os.path.getsize(filepath)
                            except Exception as e:
                                self.logger.error(f"Error backing up {filepath}: {e}")
                                stats["errors"] += 1
            else:
                # Simple copy
                shutil.copytree(source_path, backup_path, 
                              ignore=shutil.ignore_patterns(*self.config["exclude_patterns"]))
                
                # Count files
                for root, dirs, files in os.walk(backup_path):
                    stats["directories"] += len(dirs)
                    stats["files"] += len(files)
                    for file in files:
                        stats["total_size"] += os.path.getsize(os.path.join(root, file))
            
            # Verify backup
            if self.config["verify_after_backup"]:
                if self.verify_backup(source_path, backup_path):
                    self.logger.info("Backup verification passed")
                else:
                    self.logger.error("Backup verification failed")
                    raise BackupError("Backup verification failed")
            
            # Record backup in history
            backup_record = {
                "timestamp": datetime.datetime.now().isoformat(),
                "source": source_path,
                "backup_path": backup_path,
                "size": stats["total_size"],
                "files": stats["files"],
                "directories": stats["directories"],
                "errors": stats["errors"],
                "compressed": self.config["compress"]
            }
            self.backup_history.append(backup_record)
            self.save_history()
            
            self.logger.info(f"Backup completed: {stats['files']} files, "
                           f"{stats['total_size']} bytes, {stats['errors']} errors")
            
            # Clean old backups
            self.cleanup_old_backups()
            
            return backup_path
            
        except Exception as e:
            self.logger.error(f"Backup failed: {e}")
            return None
    
    def verify_backup(self, source_path: str, backup_path: str) -> bool:
        """Verify backup integrity"""
        self.logger.info("Verifying backup integrity...")
        
        if backup_path.endswith('.zip'):
            try:
                with zipfile.ZipFile(backup_path, 'r') as zipf:
                    # Test zip integrity
                    if zipf.testzip():
                        self.logger.error("ZIP file is corrupted")
                        return False
                    
                    # Get list of files in zip
                    zip_files = set(zipf.namelist())
                    
                    # Get list of source files
                    source_files = set()
                    for root, dirs, files in os.walk(source_path):
                        for file in files:
                            filepath = os.path.join(root, file)
                            if not self.should_exclude(filepath):
                                relpath = os.path.relpath(filepath, os.path.dirname(source_path))
                                source_files.add(relpath)
                    
                    # Compare
                    missing = source_files - zip_files
                    extra = zip_files - source_files
                    
                    if missing:
                        self.logger.warning(f"Missing files in backup: {missing}")
                        return False
                    
                    if extra:
                        self.logger.warning(f"Extra files in backup: {extra}")
                    
                    return True
                    
            except Exception as e:
                self.logger.error(f"Verification failed: {e}")
                return False
        else:
            # Compare directories
            return self.compare_directories(source_path, backup_path)
    
    def compare_directories(self, dir1: str, dir2: str) -> bool:
        """Compare two directories"""
        try:
            # Get file lists
            files1 = set()
            for root, dirs, files in os.walk(dir1):
                for file in files:
                    if not self.should_exclude(os.path.join(root, file)):
                        relpath = os.path.relpath(os.path.join(root, file), dir1)
                        files1.add(relpath)
            
            files2 = set()
            for root, dirs, files in os.walk(dir2):
                for file in files:
                    relpath = os.path.relpath(os.path.join(root, file), dir2)
                    files2.add(relpath)
            
            return files1 == files2
            
        except Exception as e:
            self.logger.error(f"Directory comparison failed: {e}")
            return False
    
    def restore_backup(self, backup_path: str, restore_path: str) -> bool:
        """Restore from a backup"""
        try:
            if not os.path.exists(backup_path):
                raise BackupError(f"Backup does not exist: {backup_path}")
            
            self.logger.info(f"Restoring {backup_path} to {restore_path}")
            
            # Create restore directory
            os.makedirs(restore_path, exist_ok=True)
            
            if backup_path.endswith('.zip'):
                with zipfile.ZipFile(backup_path, 'r') as zipf:
                    zipf.extractall(restore_path)
            else:
                # Copy directory contents
                for item in os.listdir(backup_path):
                    src = os.path.join(backup_path, item)
                    dst = os.path.join(restore_path, item)
                    
                    if os.path.isdir(src):
                        shutil.copytree(src, dst, dirs_exist_ok=True)
                    else:
                        shutil.copy2(src, dst)
            
            self.logger.info("Restore completed successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"Restore failed: {e}")
            return False
    
    def cleanup_old_backups(self):
        """Remove old backups exceeding max_backups"""
        max_backups = self.config["max_backups"]
        
        # Group backups by source
        backups_by_source = {}
        for record in self.backup_history:
            source = record["source"]
            if source not in backups_by_source:
                backups_by_source[source] = []
            backups_by_source[source].append(record)
        
        # Remove old backups
        for source, backups in backups_by_source.items():
            if len(backups) > max_backups:
                # Sort by timestamp
                backups.sort(key=lambda x: x["timestamp"])
                
                # Remove oldest
                to_remove = backups[:-max_backups]
                for record in to_remove:
                    try:
                        backup_path = record["backup_path"]
                        if os.path.exists(backup_path):
                            if os.path.isdir(backup_path):
                                shutil.rmtree(backup_path)
                            else:
                                os.remove(backup_path)
                            self.logger.info(f"Removed old backup: {backup_path}")
                            
                            # Remove from history
                            self.backup_history.remove(record)
                    except Exception as e:
                        self.logger.error(f"Error removing old backup: {e}")
        
        self.save_history()
    
    def list_backups(self, source: str = None) -> List[Dict]:
        """List all backups"""
        if source:
            return [b for b in self.backup_history if b["source"] == source]
        return self.backup_history
    
    def backup_report(self) -> str:
        """Generate backup report"""
        report = []
        report.append("="*60)
        report.append("BACKUP SYSTEM REPORT")
        report.append("="*60)
        report.append(f"Generated: {datetime.datetime.now().isoformat()}")
        report.append("")
        
        # Configuration
        report.append("CONFIGURATION:")
        report.append(f"  Backup directory: {self.config['backup_dir']}")
        report.append(f"  Max backups: {self.config['max_backups']}")
        report.append(f"  Compression: {self.config['compress']}")
        report.append(f"  Verify after backup: {self.config['verify_after_backup']}")
        report.append("")
        
        # Sources
        report.append("SOURCES:")
        for source in self.config["sources"]:
            if os.path.exists(source):
                size = sum(os.path.getsize(os.path.join(root, f)) 
                          for root, dirs, files in os.walk(source) 
                          for f in files)
                report.append(f"  ‚úÖ {source} ({self.format_size(size)})")
            else:
                report.append(f"  ‚ùå {source} (NOT FOUND)")
        report.append("")
        
        # Backup history
        report.append("BACKUP HISTORY:")
        if self.backup_history:
            for backup in sorted(self.backup_history, key=lambda x: x["timestamp"], reverse=True)[:10]:
                report.append(f"  {backup['timestamp']}")
                report.append(f"    Source: {backup['source']}")
                report.append(f"    Path: {backup['backup_path']}")
                report.append(f"    Size: {self.format_size(backup['size'])}")
                report.append(f"    Files: {backup['files']}")
                report.append(f"    Errors: {backup['errors']}")
                report.append("")
        else:
            report.append("  No backups yet")
        
        # Statistics
        if self.backup_history:
            total_backups = len(self.backup_history)
            total_size = sum(b["size"] for b in self.backup_history)
            total_files = sum(b["files"] for b in self.backup_history)
            total_errors = sum(b["errors"] for b in self.backup_history)
            
            report.append("STATISTICS:")
            report.append(f"  Total backups: {total_backups}")
            report.append(f"  Total data backed up: {self.format_size(total_size)}")
            report.append(f"  Total files backed up: {total_files}")
            report.append(f"  Total errors: {total_errors}")
        
        report.append("="*60)
        
        return "\n".join(report)
    
    @staticmethod
    def format_size(size_bytes: int) -> str:
        """Format file size in human-readable format"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} TB"
    
    def interactive_menu(self):
        """Interactive menu for backup system"""
        while True:
            print("\n" + "="*60)
            print("BACKUP SYSTEM - INTERACTIVE MENU")
            print("="*60)
            print("1. üìã View Configuration")
            print("2. ‚ûï Add Backup Source")
            print("3. ‚ùå Remove Backup Source")
            print("4. üíæ Create Backup")
            print("5. üìÇ List Backups")
            print("6. üîÑ Restore Backup")
            print("7. üìä Generate Report")
            print("8. ‚öôÔ∏è  Edit Configuration")
            print("9. üö™ Exit")
            print("-"*60)
            
            choice = input("Select option (1-9): ").strip()
            
            if choice == "1":
                print("\nCURRENT CONFIGURATION:")
                print(json.dumps(self.config, indent=2))
            
            elif choice == "2":
                source = input("Enter source path to backup: ").strip()
                try:
                    self.add_source(source)
                except BackupError as e:
                    print(f"Error: {e}")
            
            elif choice == "3":
                print("\nCurrent sources:")
                for i, src in enumerate(self.config["sources"], 1):
                    print(f"{i}. {src}")
                try:
                    idx = int(input("Enter source number to remove: ")) - 1
                    if 0 <= idx < len(self.config["sources"]):
                        self.remove_source(self.config["sources"][idx])
                except (ValueError, IndexError):
                    print("Invalid selection")
            
            elif choice == "4":
                if not self.config["sources"]:
                    print("No sources configured. Add a source first.")
                else:
                    print("\nSelect source to backup:")
                    for i, src in enumerate(self.config["sources"], 1):
                        print(f"{i}. {src}")
                    try:
                        idx = int(input("Enter source number: ")) - 1
                        if 0 <= idx < len(self.config["sources"]):
                            name = input("Backup name (optional): ").strip() or None
                            backup_path = self.create_backup(self.config["sources"][idx], name)
                            if backup_path:
                                print(f"‚úÖ Backup created: {backup_path}")
                    except (ValueError, IndexError):
                        print("Invalid selection")
            
            elif choice == "5":
                backups = self.list_backups()
                if backups:
                    print("\nBACKUP HISTORY:")
                    for i, backup in enumerate(backups, 1):
                        print(f"{i}. {backup['timestamp']} - {backup['source']} - {self.format_size(backup['size'])}")
                else:
                    print("No backups found")
            
            elif choice == "6":
                backups = self.list_backups()
                if not backups:
                    print("No backups available")
                else:
                    print("\nSelect backup to restore:")
                    for i, backup in enumerate(backups, 1):
                        print(f"{i}. {backup['timestamp']} - {backup['source']}")
                    
                    try:
                        idx = int(input("Enter backup number: ")) - 1
                        if 0 <= idx < len(backups):
                            backup = backups[idx]
                            restore_path = input(f"Enter restore path [default: ./restore]: ").strip() or "./restore"
                            if self.restore_backup(backup["backup_path"], restore_path):
                                print(f"‚úÖ Restored to {restore_path}")
                    except (ValueError, IndexError):
                        print("Invalid selection")
            
            elif choice == "7":
                report = self.backup_report()
                print(report)
                
                # Save report to file
                filename = f"backup_report_{datetime.datetime.now().strftime('%Y%m%d')}.txt"
                with open(filename, 'w') as f:
                    f.write(report)
                print(f"\n‚úÖ Report saved to {filename}")
            
            elif choice == "8":
                print("\nEDIT CONFIGURATION (press Enter to keep current value):")
                
                backup_dir = input(f"Backup directory [{self.config['backup_dir']}]: ").strip()
                if backup_dir:
                    self.config['backup_dir'] = backup_dir
                
                max_backups = input(f"Max backups [{self.config['max_backups']}]: ").strip()
                if max_backups:
                    self.config['max_backups'] = int(max_backups)
                
                compress = input(f"Compress backups? (True/False) [{self.config['compress']}]: ").strip()
                if compress:
                    self.config['compress'] = compress.lower() == 'true'
                
                verify = input(f"Verify after backup? (True/False) [{self.config['verify_after_backup']}]: ").strip()
                if verify:
                    self.config['verify_after_backup'] = verify.lower() == 'true'
                
                self.save_config()
                print("‚úÖ Configuration updated")
            
            elif choice == "9":
                print("Goodbye!")
                break
            
            input("\nPress Enter to continue...")

# Run the backup system
if __name__ == "__main__":
    import sys
    
    # Create backup system
    backup_system = BackupSystem()
    
    # Command line mode
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "backup":
            if len(sys.argv) > 2:
                source = sys.argv[2]
                name = sys.argv[3] if len(sys.argv) > 3 else None
                backup_system.create_backup(source, name)
            else:
                print("Usage: python backup_system.py backup <source_path> [backup_name]")
        
        elif command == "restore":
            if len(sys.argv) > 3:
                backup_path = sys.argv[2]
                restore_path = sys.argv[3]
                backup_system.restore_backup(backup_path, restore_path)
            else:
                print("Usage: python backup_system.py restore <backup_path> <restore_path>")
        
        elif command == "list":
            backups = backup_system.list_backups()
            for backup in backups:
                print(f"{backup['timestamp']} - {backup['source']} - {backup['backup_path']}")
        
        elif command == "report":
            print(backup_system.backup_report())
        
        elif command == "add":
            if len(sys.argv) > 2:
                backup_system.add_source(sys.argv[2])
            else:
                print("Usage: python backup_system.py add <source_path>")
        
        else:
            print(f"Unknown command: {command}")
    
    else:
        # Interactive mode
        try:
            backup_system.interactive_menu()
        except KeyboardInterrupt:
            print("\n\nGoodbye!")
            sys.exit(0)
üéâ Congratulations!
You've completed Week 5! You now have a solid understanding of file handling and exception management, which are crucial for building robust real-world applications.

Next Week: Week 6 - Object-Oriented Programming (OOP)





